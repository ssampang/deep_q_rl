%!TEX root = nips2015.tex
\section{Experiments}
There are three major parameters in our system. We now describe them.
\subsection{Input representation of the game}
We evaluated three different matrix representations of the Go board.
\subsubsection{One channel representation} Translating the board to a matrix directly by encoding a white stone as 255, a black stone as 0 and an empty position as 127. This was nearest to the Atari representation, but we felt that it trued to unnecessarily "compress" the data into one channel.
\subsubsection{Three channel representation} The first and second channels encode black and white stones respectively by placing a one in the matrix if there is a stone, 0 otherwise. The third channel places a one if there is a stone, zero if it is empty; this is basically the sum of first two channels and we believed it could help by more explicitly representing the illegal moves that our agent was most prone to attempt (placing pieces in occupied positions).
\subsubsection{Seven channel representation} This is inspired from Clark \& Storkey's input to the CNN which encodes information about the game itself by using the concept of liberties as shown in Figure \ref{fig:liberties}. A stone is said to have 1 liberty for every neighboring unoccupied position (in Figure 1, a black stone with 2 liberties is shown, as it has 2 neighboring white stones). If two stones of the same color are adjacent, the liberties of each stone are shared (this is depicted in the top left corner of Figure 1, where the group of three stones have a liberty count of 7 - the diagonally placed stone has a liberty of four and is not part of the group.). In this representation, we encode black positions in the first three channels by placing a one in the first channel if that stone has more than three liberties, one in the second channel if that stone has two, and one in the last channel if that stone has one liberty. The next three channels similarly encode positions of white stones. Each channel has a padding of three on all sides, so each channel of a 7 x 7 board will be of size 10 x 10. For the first six channels the padding is set to 0, whereas the seventh channel is 0 everywhere except at the padding where it is set to 1. This padding encodes the board's boundary. As it is not the focus of this paper, we leave it to the reader to find more details on the motivation for this final channel in Clark \& Storkey's paper.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{ExampleLiberties}
	\caption{Concept of Stone Liberties}
\label{fig:liberties}
\end{figure}
\\
\subsection{Architecture of the convolution neural network}
Briefly, the different network configurations we experimented with are as follows:
\\
i. Two layer CNN with one dense layer and one softmax
\\
ii. Three layer CNN with one dense layer and one softmax
\\
iii. Five layer CNN with one dense layer
\\
iV. Seven layer CNN with one dense layer
\\
In addition, we varied the filter sizes of the convolution layers.
\subsection{Parameters of Q Learning}
We also varied the experience replay size and discount factor to find the right range of values. 
\\
Each experiment was performed with the hope that our main issue was simply that of finding the right network that can approximate our Q-function. However, as we will discuss later, we believe there are more fundamental issues with our approach that prevent deep reinforcement learning from successfully tackling Go without other meaningful improvements.

\subsection{Weight tying}
Apart from the configurations mentioned, we also attempted to integrate weight tying, which was used in the by [2] to force the network to take advantage of board symmetry by tying weights together within the convolutional layers. However, we were limited by Lasagne as it allows weight tying only between filters and not within each filter; we could not find an alternative way to implement this feature in the time allotted. This was a disappointment because intuitively this kind of understanding would help in speeding up the convergence as the network would have to learn only one eighth of the filter weights. 

\subsection{Illegal Moves}
Another issue that proved to be challenging, as previously mentioned, was to find a way to prevent the agent from making illegal moves. The fundamental difference between the game of Go and Atari games is the action space is dynamic. Initially the action space is square of the board size as all the places are empty, however the space keeps changing as positions get filled or freed as stones are added or captured by the players. This resulted in a huge negative reward as the network consistently gave illegal moves and surprisingly did not learn from the negative reward. One of the reasons for this could be the constantly varying game state with no repetition. We dealt with this problem by adding a multiplicative layer at the output of the last dense layer which zeroed out the Q values of illegal moves.  
