%!TEX root = nips2015.tex
\section{Analysis and Future Work}
In this section, we offer hypotheses that attempt to explain why our agent was not able to learn an appropriate Q-network for Go. First, we simply could not generate a sufficient number of quality training examples. By "quality", we refer to the fact that our network would generate a large number of illegal moves, and each one would be added to the data set as a training example with a reward of -1 in the hopes that it would allow our network to intrinsically learn to avoid illegal moves. However, this led to a disproportionately large number of illegal move examples being added to our training set, which meant that a large number of training examples did not involve any notion of strategy in the game. Concretely, we would sometimes see illegal moves account for upwards of 50\% of the (state,action,reward) sequences added from each game.

Even if we disregard this, the speed with which we were able to generate training examples was simply far from sufficient. Considering that [2] trained on 16 million examples, and that, assuming approximately 50 moves per game on a board of size 7, it took us 3 days to generate 100,000 examples, it is simply infeasible to expect to generate a data set of comparable size. We must then rely on using training examples in a more efficient way, or we must avoid training a network from scratch.

One possible efficiency performed by [2] was to tie the weights of every convolutional layer such that in the end, each layer had only 1/8 the weights. The intuition behind this is that for every possible reflection and rotation (there are a total of 8 possible combinations), the next best action should be identical after the same reflection/rotation is performed. Thus, each training example can be perturbed to form 8 training examples. Instead of supersampling the training set, it is far more efficient to enforce every convolutional layer to be identical across all possible reflections and rotations. We tried very hard to implement this modification, but we were not able to do so because Lasagne/Theano does not possess a straightforward method to tie weights across the same layer. To the extent of our knowledge, Lasagne only allows weight sharing between layers.

Another possible modification is to use the network trained in [2], and simply improve on it using reinforcement learning. We also spent considerable time attempting to do this. However, 